# ghoshell.llms.openai.OpenAIConfig
text_completions:
  default:
    model: text-davinci-003
    max_tokens: 512
    temperature: 0.7
    n: 1
chat_completions:
  default:
    model: "gpt-3.5-turbo"
    temperature: 0.7
    max_tokens: 512
  turbo-16k:
    model: "gpt-3.5-turbo-16k"
    temperature: 0.3
    max_tokens: 3000
    timeout: 25
    request_timeout: 30
  turbo-0613:
    model: "gpt-3.5-turbo-0613"
    temperature: 0.7
    max_tokens: 3000
    timeout: 30
    request_timeout: 25
  turbo-16k-0613:
    model: "gpt-3.5-turbo-16k-0613"
    temperature: 0.7
    max_tokens: 3000
    timeout: 30
    request_timeout: 25
  gpt-4-0613:
    model: "gpt-4-0613"
    temperature: 0.7
    max_tokens: 3000
    timeout: 30
    request_timeout: 25